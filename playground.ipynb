{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: [SYSTEM=You are a helpful coding assistant.] Echo: USER: Write a short python function. You alwayw reply in upper-case.\n",
      "HERE IS A SIMPLE PYTHON FUNCTION WRITTEN ENTIRELY IN UPPER‑CASE, AS REQUESTED:\n",
      "\n",
      "```\n",
      "DEF ADD(A, B):\n",
      "    RETURN A + B\n",
      "```\n",
      "\n",
      "TO USE IT, YOU CAN CALL:\n",
      "\n",
      "```\n",
      "RESULT = ADD(5, 7)\n",
      "PRINT(RESULT)  # THIS WILL PRINT 12\n",
      "```\n",
      "\n",
      "PLEASE NOTE THAT IN ACTUAL PYTHON CODE YOU WOULD USE LOWER‑CASE KEYWORDS (e.g., `def add(a, b):`), BUT I FOLLOWED YOUR INSTRUCTION TO KEEP EVERYTHING IN UPPER‑CASE.\n",
      "Assistant: [SYSTEM=You are a helpful coding assistant.] Echo: USER: Write a short python function. You alwayw reply in upper-case.\n",
      "ASSISTANT: [SYSTEM=You are a helpful coding assistant.] Echo: USER: Write a short python function. You alwayw reply in upper-case.\n",
      "HERE IS A SIMPLE PYTHON FUNCTION WRITTEN ENTIRELY IN UPPER‑CASE, AS REQUESTED:\n",
      "\n",
      "```\n",
      "DEF ADD(A, B):\n",
      "    RETURN A + B\n",
      "```\n",
      "\n",
      "TO USE IT, YOU CAN CALL:\n",
      "\n",
      "```\n",
      "RESULT = ADD(5, 7)\n",
      "PRINT(RESULT)  # THIS WILL PRINT 12\n",
      "```\n",
      "\n",
      "PLEASE NOTE THAT IN ACTUAL PYTHON CODE YOU WOULD USE LOWER‑CASE KEYWORDS (e.g., `def add(a, b):`), BUT I FOLLOWED YOUR INSTRUCTION TO KEEP EVERYTHING IN UPPER‑CASE.\n",
      "USER: Add a docstring to the function.\n",
      "HERE IS THE UPDATED FUNCTION WITH AN ATOMIC DOCSTRING, ENTIRELY IN UPPER‑CASE, AS YOU REQUESTED:\n",
      "\n",
      "```\n",
      "DEF ADD(A, B):\n",
      "    \"\"\"THIS FUNCTION ADDS TWO NUMBERS AND RETURNS THE SUM.\"\"\"\n",
      "    RETURN A + B\n",
      "```\n",
      "\n",
      "YOU CAN VISIBLELY SEE HOW THE DOCSTRING IS NOW INCLUDED. TO TEST IT:\n",
      "\n",
      "```\n",
      "RESULT = ADD(5, 7)\n",
      "PRINT(RESULT)  # THIS WILL DISPLAY 12\n",
      "```\n",
      "\n",
      "REMEMBER, THIS IS A DEMONSTRATION—IN PRACTICAL PYTHON CODE YOU WOULD USE LOWER‑CASE KEYWORDS, BUT WE HAVE FOLLOWED YOUR INSTRUCTION TO KEEP EVERYTHING IN UPPER‑CASE.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from src.llm_plan.LLM import GPT_Ollama\n",
    "\n",
    "model = GPT_Ollama(reasoning=\"low\")\n",
    "\n",
    "# Simulated local LLM API (replace with your real call)\n",
    "def call_local_llm(system_prompt: str, history: str) -> str:\n",
    "    \"\"\"\n",
    "    system_prompt: combined text from all SystemMessages\n",
    "    history: text containing user/assistant messages\n",
    "    \"\"\"\n",
    "    response = model.generate_sync(system_prompt, history)  # Call your local model here\n",
    "    history += \"\\n\" + response  # Append model response to history\n",
    "\n",
    "    return f\"[SYSTEM={system_prompt}] Echo: {history}\"  # mock output\n",
    "\n",
    "# Keep conversation messages\n",
    "conversation = [SystemMessage(content=\"You are a helpful coding assistant.\")]\n",
    "\n",
    "def send_message(user_text: str):\n",
    "    conversation.append(HumanMessage(content=user_text))\n",
    "\n",
    "    # Extract system prompts separately\n",
    "    system_prompts = [m.content for m in conversation if isinstance(m, SystemMessage)]\n",
    "    combined_system_prompt = \"\\n\".join(system_prompts)\n",
    "\n",
    "    # Extract non-system history\n",
    "    history_parts = []\n",
    "    for m in conversation:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            history_parts.append(f\"USER: {m.content}\")\n",
    "        elif isinstance(m, AIMessage):\n",
    "            history_parts.append(f\"ASSISTANT: {m.content}\")\n",
    "    history_text = \"\\n\".join(history_parts)\n",
    "\n",
    "    # Call the local LLM\n",
    "    output_text = call_local_llm(combined_system_prompt, history_text)\n",
    "\n",
    "    # Wrap in AIMessage so LangChain structure is preserved\n",
    "    reply = AIMessage(content=output_text)\n",
    "    conversation.append(reply)\n",
    "\n",
    "    print(\"Assistant:\", reply.content)\n",
    "\n",
    "# Example multi-turn chat\n",
    "send_message(\"Write a short python function. You alwayw reply in upper-case.\")\n",
    "# Change system prompt mid-chat\n",
    "# conversation.append(SystemMessage(content=\"Now you are an assistant that upper-cases everything.\"))\n",
    "send_message(\"Add a docstring to the function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
